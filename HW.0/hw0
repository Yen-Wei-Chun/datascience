# 輸入需要的packages, module

import sys
import pickle # 保存序列化物件
import requests # 抓取網頁資料
from datetime import datetime
from bs4 import BeautifulSoup # 解析網頁HTML碼

# 處理新聞日期
def get_date(news_block_node):
    date_string = news_block_node.find(class_="news_date").string.split('|')[0][2:-1]
    return(datetime.strptime(date_string, '%Y.%m.%d').strftime('%Y-%m-%d')) # 將日期回傳成字串

# 處理新聞標題
def get_title(news_block_node):
    return news_block_node.find(class_="news_title").a.string

# 處理新聞連結
def get_link(news_block_node):
    return news_block_node.find(class_="news_title").a.get("href") #取得超連結網址

# 取得內文
def get_content(link):
    r = requests.get(link) # 獲取網頁原始碼
    r.encoding = "UTF-8"
    soup = BeautifulSoup(r.text, 'html.parser') # BeautifulSoup解析HTML程式碼
    article_node = soup.find(itemprop = 'articleBody')
    article = article_node.get_text()
    return article.replace("\n", "")

# 彙整新聞資訊(日期、標題、連結、內文)，存成dictionary
def get_news_info(each_news):
    date = get_date(each_news)
    title = get_title(each_news)
    link = get_link(each_news)
    content = get_content(link)

    info = {'date': date, 'title': title, "link": link, "content": content}
    return (info)

# 蒐集網頁內的新聞(dict)，存成list
def get_page_news(page_url):
    r = requests.get(page_url)
    r.encoding = "UTF-8"

    soup = BeautifulSoup(r.text, 'html.parser')
    news_blocks = soup.find_all(class_="news-list-item clearfix")

    news = []
    for each_news in news_blocks:
        try:
            news_info = get_news_info(each_news)
        #             print(get_title(each_news))
        except:
            #             print('-------{}-------'.format())
            pass

        news.append(news_info)
    return (news)

# 重複擷取新聞資料存成list
def get_new_talk_news(from_page=1, end_page=270, url="https://newtalk.tw/news/subcategory/2/政治/"):
    print("page_number from {} to {}".format(from_page, end_page - 1))
    data = []
    for page_number in range(from_page, end_page):
        print("page_number: {}".format(page_number))
        data = data + get_page_news(url + str(page_number))

    print('done')
    return (data)

# 將所有資料命名為data
data = get_new_talk_news(from_page=1, end_page=270)

# 開啟檔案
# 把data存進handle
sys.setrecursionlimit(100000)
with open('/Users/gillian0531/Downloads/new_talk.pkl', 'wb') as handle:
    pickle.dump(data, handle)
